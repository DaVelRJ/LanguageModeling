# Word Embeddings with Word2Vec
Being able to represent a word as a series of numerical values is essential to the large language modeling done using the transformer architecture. Word2Vec makes this possible extracting the word embeddings creating during the skip-gram or continuous bag-of-words(CBOW) process. More info can be found <a href='https://www.youtube.com/watch?v=gQddtTdmG_8' target="_blank">here</a>.
<br>
Adapted from <a href='https://www.kaggle.com/code/jihyeseo/word2vec-gensim-play-look-for-similar-words' target="_blank">Kaggle</a> and <a href='https://towardsdatascience.com/understanding-word2vec-embedding-in-practice-3e9b8985953' target="_blank">TowardsDataScience</a>. <br> Written By: Da'Vel Johnson
